constants:
  # ========== VISION ==========
  vision_embed_dim: 1024
  vision_num_blocks: 24
  vision_num_heads: 8
  # ========== TEXT ==========
  text_embed_dim: 768
  text_num_blocks: 12
  text_num_heads: 6
  # ========== CLIP ==========
  learnable_temperature: true
  # ========== DATA ==========
  seed: 0
  context_length: 77
  text_vocab_size: 49408
  img_size: 224
  batch_size_per_gpu: 128
  patch_size: 14
  num_patches: 256
  # ========== TOKENIZER ==========
  tokenizer:
    _target_: l3m.model.layers.tokenizers.HFTokenizer
    tokenizer_name: openai/clip-vit-base-patch32
    context_length: ${constants.context_length}
    max_seq_len: ${constants.context_length}
  # ========== TRANSFORMS ==========
  train_transform:
    _target_: torchvision.transforms.Compose
    transforms:
    - _target_: torchvision.transforms.RandomResizedCrop
      size: ${constants.img_size}
      scale: [0.4, 1.0]
      interpolation: 3   # bicubic
    - _target_: torchvision.transforms.RandomHorizontalFlip
    - _target_: torchvision.transforms.ToTensor
    - _target_: torchvision.transforms.Normalize
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
  eval_transform:
    _target_: torchvision.transforms.Compose
    transforms:
    - _target_: torchvision.transforms.Resize
      size: 256      # crop ratio = 0.875
      interpolation: 3
    - _target_: torchvision.transforms.CenterCrop
      size: ${constants.img_size}
    - _target_: torchvision.transforms.ToTensor
    - _target_: torchvision.transforms.Normalize
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

# ==============================================================
# DATA
# ==============================================================
data:
  # ========== TRAIN ==========
  train:
    dataset:
      _target_: l3m.data.huggingface.multimodal.SimpleImageTextDataset
      tokenizer: ${constants.tokenizer}
      transforms: ${constants.train_transform}
    dataloader:
      _target_: torch.utils.data.DataLoader
      _partial_: true
      batch_size: ${constants.batch_size_per_gpu}
      num_workers: 4
  # ========== VALIDATION ==========
  validation:
    dataset:
      _target_: l3m.data.huggingface.multimodal.SimpleImageTextDataset
      tokenizer: ${constants.tokenizer}
      transforms: ${constants.train_transform}
    dataloader:
      _target_: torch.utils.data.DataLoader
      _partial_: true
      batch_size: ${constants.batch_size_per_gpu}
      num_workers: 4

optim:
  grad_clip:
  gradient_accumulation_steps: 1
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1e-3
    betas:
    - 0.9
    - 0.95
    eps: 1e-08
    weight_decay: 0.2
    fused: true
  wd_exclude:
  - '*bias*'
  - '*pos_embed*'
  - '*positional_embedding*'
  - '*norm*'
  - '*gamma*'
  - '*temperature*'
  - '*log_logit_scale*'
  scheduler:
    _target_: fvcore.common.param_scheduler.CompositeParamScheduler
    schedulers:
    - _target_: fvcore.common.param_scheduler.LinearParamScheduler
      start_value: 1e-6
      end_value: ${optim.optimizer.lr}
    - _target_: fvcore.common.param_scheduler.CosineParamScheduler
      start_value: ${optim.optimizer.lr}
      end_value: 1e-6
    absolute_lengths: true
    interval_scaling:
    - rescaled
    - rescaled
    lengths:
    - 6_250
    - 118_750

# ==============================================================
# MODEL
# ==============================================================
model:
  checkpoint:
  meta_model:
    _target_: l3m.model.meta_models.InSeriesMetaModels
    models:
      # ========== IMAGE ENCODER ==========
      image_encoder:
        _target_: l3m.model.meta_models.MetaModel
        preprocessor:
          _target_: l3m.model.utils.MultiBlock
          blocks:
          - _target_: l3m.model.preprocessors.vision.ViTPreprocessor
            read_key: image
            write_key: image_tokens
            patchifier:
              _target_: l3m.model.preprocessors.vision.PatchEmbed
              img_size: ${constants.img_size}
              patch_size: ${constants.patch_size}
              in_chans: 3
              embed_dim: ${constants.vision_embed_dim}
              norm_layer:
                _target_: l3m.model.layers.normalization.RMSNorm
                _partial_: true
                eps: 1e-5
            pos_embed_type: absolute
            drop_patches: false
            cls_token: true
        trunk:
          _target_: l3m.model.trunks.transformer.Transformer
          read_key: image_tokens
          write_key: image_tokens
          embed_dim: ${constants.vision_embed_dim}
          num_blocks: ${constants.vision_num_blocks}
          mlp_ratio: 4
          ffn_target:
            _target_: l3m.model.layers.ffn.SwiGLUFFN
            _partial_: true
          norm_layer:
            _target_: l3m.model.layers.normalization.RMSNorm
            _partial_: true
            eps: 1e-5
          attn_target:
            _target_: l3m.model.layers.attention.GenericAttention
            _partial_: true
            dim: ${constants.vision_embed_dim}
            num_heads: ${constants.vision_num_heads}
            qkv_bias: false
            is_causal: false
          weight_init_style: xavier_uniform
          post_trunk_norm: true
          use_bias: false
        postprocessor:
          _target_: l3m.model.postprocessors.select.ExtractCLS
          index: 0
          read_key: image_tokens
          write_key: image_embed
        head:
          _target_: l3m.model.heads.clip_head.CLIPHead
          read_key: image_embed
          write_key: image_embed
          in_features: ${constants.vision_embed_dim}
          out_features: ${constants.text_embed_dim}
          use_bias: false
          apply_l2_norm: true
          apply_temperature: true
          init_temperature: 0.07
          learnable_temperature: ${constants.learnable_temperature}
      # ========== TEXT ENCODER ==========
      text_encoder:
        _target_: l3m.model.meta_models.MetaModel
        preprocessor:
          _target_: l3m.model.preprocessors.vision.TextPreprocessor
          read_key: text
          write_key: text_tokens
          vocab_size: ${constants.text_vocab_size}
          embed_dim: ${constants.text_embed_dim}
          context_length: ${constants.context_length}
          cls_token: false
        trunk:
          _target_: l3m.model.trunks.transformer.Transformer
          read_key: text_tokens
          write_key: text_tokens
          embed_dim: ${constants.text_embed_dim}
          num_blocks: ${constants.text_num_blocks}
          mlp_ratio: 4
          norm_layer:
            _target_: l3m.model.layers.normalization.RMSNorm
            _partial_: true
            eps: 1e-5
          ffn_target:
            _target_: l3m.model.layers.ffn.SwiGLUFFN
            _partial_: true
          attn_target:
            _target_: l3m.model.layers.attention.GenericAttention
            _partial_: true
            dim: ${constants.text_embed_dim}
            num_heads: ${constants.text_num_heads}
            qkv_bias: false
            is_causal: true
          weight_init_style: xavier_uniform
          post_trunk_norm: true
          use_bias: false
        postprocessor:
          _target_: l3m.model.postprocessors.select.ExtractEOS
          read_key: text_tokens
          write_key: text_embed
          eos_token_id: 49407 # should be set
        head:
          _target_: l3m.model.heads.clip_head.CLIPHead
          read_key: text_embed
          write_key: text_embed
          in_features: ${constants.text_embed_dim}
          out_features: ${constants.text_embed_dim}
          use_bias: false
          apply_l2_norm: true
          apply_temperature: false

# ==============================================================
# LOSS
# ==============================================================
loss:
  _target_: l3m.loss.clip_loss.CLIPContrastiveLoss
  image_read_key: image_embed
  text_read_key: text_embed
  gather_with_grad: true
val_loss:
  _target_: l3m.loss.clip_loss.CLIPContrastiveLoss
  image_read_key: image_embed
  text_read_key: text_embed
  gather_with_grad: false

# ==============================================================
# METRICS
# ==============================================================
metrics:
  _target_: l3m.metrics.MetricsComputer
  metrics:
  - _target_: l3m.metrics.accuracy.CLIPAccuracy
    read_key: image_embed
    topk: [1, 5]
    tokenizer: ${constants.tokenizer}
    imagenet_class_names_path: data/assets/imagenet_class_names.pkl

# ==============================================================
# EXPERIMENT
# ==============================================================
experiment:
  start_iteration: 0
  total_iterations: 125_000
  ckpt_save_freq: 10_000
  test_frequency: 10_000
  torch_compile: true
  dtype: bfloat16
  output_dir:
  device: cuda
  find_unused_parameters: false
  seed: ${constants.seed}
  dist_eval: true
  distributed: false  # will be automatically enabled
  world_size: 1  # will be automatically updated
  dist_url: env://
  eval: false
  resume: # Should be populated with a ckpt path on job resubmission
  amp_enabled: true
  no_sync_gradient_accumulation: true
  nccl_timeout_mins: 120
  fsdp:
    sharding_strategy: NO_SHARD
    param_dtype: bf16
    reduce_dtype: bf16
    buffer_dtype: fp32
    fsdp_activation_checkpointing: false
    fsdp_layers_to_wrap:
    - Block
    - DecoderBlock
    fsdp_layers_to_activation_checkpoint:
    - Block
    - DecoderBlock
    fsdp_ignored_modules:
    - ''

wandb:
  use_wandb: true
  watch_freq: 500
  project: l3m
  tags: [clip]
